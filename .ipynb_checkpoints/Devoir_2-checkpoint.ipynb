{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/BaichooSiddharth/Devoir_1_Data_Science/blob/master/Devoir_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 307,
     "status": "ok",
     "timestamp": 1608077716555,
     "user": {
      "displayName": "Siddharth Baichoo",
      "photoUrl": "",
      "userId": "10036096615326393336"
     },
     "user_tz": 300
    },
    "id": "NuP-C_VQ_a7p"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import zipfile\n",
    "\n",
    "#QUESTION 1)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.models import load_model\n",
    "\"\"\"\n",
    "Les imports pour entraîner un réseau de neurones décommenter pour réentraîner (au besoin)\n",
    "\n",
    "from keras.layers import Dense, Dropout, Conv2D, MaxPool2D, Flatten\n",
    "from keras.utils import np_utils\n",
    "\"\"\"\n",
    "\n",
    "#QUESTION 2)\n",
    "\n",
    "#QUESTION 3)\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "#QUESTION 4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EQGbj8qdvsSE"
   },
   "source": [
    " **LOAD DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "executionInfo": {
     "elapsed": 27855,
     "status": "ok",
     "timestamp": 1608077166429,
     "user": {
      "displayName": "Siddharth Baichoo",
      "photoUrl": "",
      "userId": "10036096615326393336"
     },
     "user_tz": 300
    },
    "id": "3gJtsI1Oa9CH",
    "outputId": "7dec4500-48cc-4642-91a3-437187122695"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nles données se trouvent dans le drive des étudiants dans une version zip\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n",
    "with zipfile.ZipFile(\"IFT_3700/PATCH.amat.zip\", 'r') as zip_ref:\n",
    "    zip_ref.extractall(\"IFT_3700\")\n",
    "\n",
    "\"\"\"\n",
    "les données se trouvent dans le drive des étudiants dans une version zip\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f1gVvBfO5C6q"
   },
   "source": [
    "## Question 1\n",
    "\n",
    "Cette question est une compétition entre les équipes et sera évaluée en fonction de la précision du classifieur produit par chaque équipe. Les données sont contenues dans le fichier PATCH.amat et il s’agit de 50000 images (28 x 28) en noir et blanc codées en binaire. Les images sont abstraites et appartiennent à deux catégories distinctes. Le fichier contient une image par ligne et chaque ligne commence par 784=28 * 28 bits associés à la couleur des pixels et suivi d’un bit représentant la classe. Le codage du fichier est lisible, mais nécessite un prétraitement pour être utilisé. L’équipe doit produire un classifieur qui sera mis en production et évalué sur des données fraîches non disponibles à l’équipe (mais qui ont exactement la même distribution). Les équipes avec une solution minimalement raisonnable seront classées en ordre de précision et le rang sera transformé en note variant de 10 à 25 sur 25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 26795,
     "status": "ok",
     "timestamp": 1608077216749,
     "user": {
      "displayName": "Siddharth Baichoo",
      "photoUrl": "",
      "userId": "10036096615326393336"
     },
     "user_tz": 300
    },
    "id": "iqggEp7g5TgR"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "On va implémenter un CNN pour la classification des images code basé de https://www.analyticsvidhya.com/blog/2020/02/learn-image-classification-cnn-convolutional-neural-networks-3-datasets/\n",
    "'''\n",
    "\n",
    "data = np.loadtxt('IFT_3700/PATCH.amat')\n",
    "\n",
    "X_train, X_test = train_test_split(data, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 310,
     "status": "ok",
     "timestamp": 1608077232135,
     "user": {
      "displayName": "Siddharth Baichoo",
      "photoUrl": "",
      "userId": "10036096615326393336"
     },
     "user_tz": 300
    },
    "id": "pNboPOtaDVck"
   },
   "outputs": [],
   "source": [
    "X_value_train = X_train[:, 1:]\n",
    "X_labels_train = X_train[:, 0]\n",
    "X_value_test = X_test[:, 1:]\n",
    "X_labels_test = X_test[:, 0]\n",
    "X_value_train = X_value_train.reshape(X_value_train.shape[0], 28, 28, 1)\n",
    "X_value_test = X_value_test.reshape(X_value_test.shape[0], 28, 28, 1)\n",
    "n_classes = len(np.unique(X_labels_test))\n",
    "Y_train = np_utils.to_categorical(X_labels_train, n_classes)\n",
    "Y_test = np_utils.to_categorical(X_labels_test, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before one-hot encoding:  (40000,)\n",
      "Shape after one-hot encoding:  (40000, 2)\n",
      "Epoch 1/10\n",
      "313/313 [==============================] - 8s 23ms/step - loss: 0.1799 - accuracy: 0.9281 - val_loss: 0.0459 - val_accuracy: 0.9840\n",
      "Epoch 2/10\n",
      "313/313 [==============================] - 7s 23ms/step - loss: 0.0429 - accuracy: 0.9845 - val_loss: 0.0347 - val_accuracy: 0.9881\n",
      "Epoch 3/10\n",
      "313/313 [==============================] - 7s 23ms/step - loss: 0.0298 - accuracy: 0.9897 - val_loss: 0.0334 - val_accuracy: 0.9883\n",
      "Epoch 4/10\n",
      "313/313 [==============================] - 7s 23ms/step - loss: 0.0255 - accuracy: 0.9906 - val_loss: 0.0357 - val_accuracy: 0.9890\n",
      "Epoch 5/10\n",
      "313/313 [==============================] - 7s 23ms/step - loss: 0.0208 - accuracy: 0.9932 - val_loss: 0.0325 - val_accuracy: 0.9902\n",
      "Epoch 6/10\n",
      "313/313 [==============================] - 8s 25ms/step - loss: 0.0152 - accuracy: 0.9951 - val_loss: 0.0368 - val_accuracy: 0.9898\n",
      "Epoch 7/10\n",
      "313/313 [==============================] - 8s 25ms/step - loss: 0.0149 - accuracy: 0.9957 - val_loss: 0.0317 - val_accuracy: 0.9911\n",
      "Epoch 8/10\n",
      "313/313 [==============================] - 7s 24ms/step - loss: 0.0171 - accuracy: 0.9946 - val_loss: 0.0321 - val_accuracy: 0.9905\n",
      "Epoch 9/10\n",
      "313/313 [==============================] - 8s 24ms/step - loss: 0.0112 - accuracy: 0.9966 - val_loss: 0.0330 - val_accuracy: 0.9916\n",
      "Epoch 10/10\n",
      "313/313 [==============================] - 7s 22ms/step - loss: 0.0088 - accuracy: 0.9973 - val_loss: 0.0321 - val_accuracy: 0.9913\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x296e0ae0250>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Code pour entraîner le réseau de neurones et le sauvegarder dans le directoire\n",
    "\n",
    "# building a linear stack of layers with the sequential model\n",
    "model = Sequential()\n",
    "# convolutional layer\n",
    "model.add(Conv2D(25, kernel_size=(3,3), strides=(1,1), padding='valid', activation='relu', input_shape=(28,28,1)))\n",
    "model.add(MaxPool2D(pool_size=(1,1)))\n",
    "# flatten output of conv\n",
    "model.add(Flatten())\n",
    "# hidden layer\n",
    "model.add(Dense(16, activation='relu'))\n",
    "# output layer\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "# compiling the sequential model\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
    "\n",
    "# training the model for 10 epochs\n",
    "model.fit(X_value_train, Y_train, batch_size=128, epochs=10, validation_data=(X_value_test, Y_test))\n",
    "\n",
    "model.save(\"my_model\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6144,
     "status": "ok",
     "timestamp": 1608077979526,
     "user": {
      "displayName": "Siddharth Baichoo",
      "photoUrl": "",
      "userId": "10036096615326393336"
     },
     "user_tz": 300
    },
    "id": "arlcpkeZu7bM",
    "outputId": "3920ed88-6219-49ef-ca69-6c9c4d3c45c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before one-hot encoding:  (40000,)\n",
      "Shape after one-hot encoding:  (40000, 2)\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 0.0231 - accuracy: 0.9921\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x297021cf730>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#modèle reconstruit qui sera utilisé pour les tests\n",
    "reconstructed_model = load_model(\"my_model\")\n",
    "reconstructed_model.fit(X_value_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "executionInfo": {
     "elapsed": 581,
     "status": "ok",
     "timestamp": 1608078008335,
     "user": {
      "displayName": "Siddharth Baichoo",
      "photoUrl": "",
      "userId": "10036096615326393336"
     },
     "user_tz": 300
    },
    "id": "HLCXHUNf_8uC",
    "outputId": "e2c51780-0c88-4d0c-f44a-a3161b584bab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_model\\assets\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x297000277f0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(np.reshape(X_value_test[0], (28, 28)), cmap=\"gray\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A3h-rEXLUtJs"
   },
   "source": [
    "L'accuracy de notre CNN est arrivé à environ 98.97 %.  Pour tester de nouvelles données, il faut juste remplacer les valeurs de X_value_test dans \"reconstructed_model.fit(X_value_test, Y_test)\" par les valeurs que l'on veut tester et on peut donner les nouveaux labels de notre nouvel ensemble de test en les mettant à la place de Y_test "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DhYRWXkbWCyi"
   },
   "source": [
    "## Question 2 \n",
    "Imaginez qu’on vous donne un fichier contenant des données de nature astronomique. Il s’agit de données concernant 6500 milliards d’étoiles. Les données (dépassant la centaine de téraoctets) sont réparties sur 300 serveurs avec un accès rapide au disque et une bonne capacité de calcul. La connexion entre les serveurs est rapide, mais pas exceptionnelle.\n",
    "\n",
    "Les données pour chaque étoile comporte, la position dans l’espace (x,y,z), la luminosité apparente de l’étoile et sa catégorie (10 catégories possibles). Aussi, pour chaque étoile une liste de ses caractéristiques physiques représentées par 22 nombres réels est incluse.\n",
    "\n",
    "Proposez une approche distribuée qui permet de répondre aux questions suivantes et expliquez en détail toute la démarche permettant leur résolution.\n",
    "Trouvez les 1000 paires d’étoiles les plus proches (distance euclidienne de la position).\n",
    "Comptez combien d'étoiles il y a dans chaque catégorie.\n",
    "Produisez un classifieur qui, étant donné le vecteur de caractéristiques (22 nombres réels), prédit la catégorie de l’étoile.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4j7R0v4YW7M_"
   },
   "source": [
    "**1. Trouver les 1000 paires**\n",
    "\n",
    "Premièrement il faut trouver les 1000 étoiles les plus proches de la terre sur chacun des 300 serveurs (pas moins car il se peut que les 1000 étoiles les plus proches se trouvent sur le même serveur).  Puis on envoie les 1000 étoiles les plus proches sur chacun des serveurs (3000000 d'étoiles <<< 6.5 trilliards étoiles) sur une machine avec une assez grande capacité puis on choisit les 1000 les plus proches parmi ces 3000000 étoiles. (Distance euclidienne = sqrt(x^2 + y^2 + z^3))\n",
    "\n",
    "**2. Compter le nombre d'étoiles dans chaque catégorie**\n",
    "\n",
    "Premièrement on compte le nombre de labels différents sur chacun des serveurs (comme on a 10 catégories ce sera plus simple après cela). Puis on envoie les résultats obtenus sur chacun des serveurs dans une seule machine (on a 3000 valeurs en tout, 10 pour chacun des serveurs donc le problème de communication est moindre). Comme cela on peut compter le nombre d'occurence de chacune des type d'étoiles\n",
    "\n",
    "**3. Classifieur pour les 22 caractéristiques**\n",
    "\n",
    "\n",
    "On entraîne chacun des serveurs avec un différent modèle (70% entrainement, 15% validation, 15% test) (pas besoin de 300 modèles différents mais on fait cela pour voir quel modèle correspondra mieux au jeu de données). Puis on choisit les 11 meilleurs modèles parmi les serveurs. Puis pour une nouvelle étoile on fait passer sur les modèles puis on fait un vote de majorité (11 classes pour couper au cas ou on a des égalités) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5g9SPPWEWJJ7"
   },
   "source": [
    "## Question 3\n",
    "\n",
    "Expliquez en détail comment utiliser un classifieur binaire, capable d’apprendre à effectuer la classification de deux catégories, pour réaliser la classification dans un contexte ou plusieurs catégories doivent être distinguées. Considérez le cas à 3, 25, 12500 catégories et faite le contraste entre les différentes approches étudiées et le nombre de catégories.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D5knwl0J7xOs"
   },
   "source": [
    "Si nous nous retrouvons avec 3, 25, 12500, nous avons un problème de classification multi-classe donc non binaire.\n",
    "\n",
    "Mais nous pouvons réduire ce problème en problème de classfication binaire en utilisant la technique *One vs. All* . On transforme **N-classes** en **N modèles de classifcations binaires**\n",
    "\n",
    "Dans la classification one-vs-All, pour l'ensemble de données des instances de classe N, nous devons générer les modèles de classificateurs N-binaires. Le nombre d'étiquettes de classe présentes dans l'ensemble de données et le nombre de classificateurs binaires générés doivent être identiques.\n",
    "\n",
    "Dans le cas de **3 catégories**, on a l'illustration ci-dessous:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 320
    },
    "executionInfo": {
     "elapsed": 325647,
     "status": "error",
     "timestamp": 1607836383906,
     "user": {
      "displayName": "Siddharth Baichoo",
      "photoUrl": "",
      "userId": "10036096615326393336"
     },
     "user_tz": 300
    },
    "id": "QgW0XqiwCUq_",
    "outputId": "0f0618d5-fef7-43c4-b8f2-fdeeec8296ac"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-dd70678ef2ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmpimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"svm.png\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Exemple de One-vs-all\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mimread\u001b[0;34m(fname, format)\u001b[0m\n\u001b[1;32m   1472\u001b[0m             \u001b[0mfd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1473\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_png\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_png\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1474\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_file_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1475\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_png\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_png\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1476\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/cbook/__init__.py\u001b[0m in \u001b[0;36mopen_file_cm\u001b[0;34m(path_or_file, mode, encoding)\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mopen_file_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m     \u001b[0;34mr\"\"\"Pass through file objects and context-manage `.PathLike`\\s.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 418\u001b[0;31m     \u001b[0mfh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_filehandle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    419\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mopened\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mfh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/cbook/__init__.py\u001b[0m in \u001b[0;36mto_filehandle\u001b[0;34m(fname, flag, return_opened, encoding)\u001b[0m\n\u001b[1;32m    401\u001b[0m             \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbz2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBZ2File\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m             \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m         \u001b[0mopened\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'seek'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'svm.png'"
     ]
    }
   ],
   "source": [
    "plt.imshow(mpimg.imread(\"svm.png\"))\n",
    "plt.title(\"Exemple de One-vs-all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vWWBb5GHrjH1"
   },
   "source": [
    "Comme le montre l'image ci-dessus, considérons que nous avons trois classes, par exemple, le type 1 pour le vert, le type 2 pour le bleu et le type 3 pour le rouge.\n",
    "\n",
    "Maintenant, comme on vous l'a dit plus tôt, nous devons générer le même nombre de classificateurs que les étiquettes de classe sont présentes dans l'ensemble de données, nous devons donc créer ici trois classificateurs binaires pour trois classes respectives.\n",
    "\n",
    "\n",
    "\n",
    "*   Classificateur 1:- [Vert] vs [Rouge, Bleu]\n",
    "* Classificateur 2 : [Bleu] vs [Vert, Rouge]\n",
    "* Classificateur 3 : [Rouge] vs [Bleu, Vert]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J2PU8Ca8uqKw"
   },
   "source": [
    "Nous pourrons maintenant implémenter un algorithme pour la classification binaire (e.g.: Support de vecteurs machine) pour chaque Classificateur *i, i = 1,2,3*.\n",
    "\n",
    "Cette approche exige que chaque modèle prévoie une probabilité d'appartenance à une classe ou un score de type probabiliste. L'argmax de ces scores (indice de classe avec le score le plus élevé) est ensuite utilisé pour prédire une classe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ApZDFlbbWzfp"
   },
   "source": [
    "## Question 4\n",
    "Faites une analyse des patrons existants dans le jeu de données adulte. Il est possible que le regroupement de valeurs pour certaines caractéristiques donne des résultats intéressants. Vous devez aussi discuter de l’aspect éthique concernant l’utilisation des patrons obtenus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jW_0jDmUW10Q"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Devoir_2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
